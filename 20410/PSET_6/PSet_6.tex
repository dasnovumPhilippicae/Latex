\documentclass{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{verbatim}
\usepackage{systeme}
\usepackage{mathrsfs}
\newcommand{\R}{\mathbb{R}}  
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Pm}{\mathbb{P}}
\newcommand{\ma}[1]{\mathbf{#1}}
\newcommand{\ent}{\vspace{0.5cm}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}
\newenvironment{answer}{\begin{proof}[Solution]}{\end{proof}}

\linespread{1.5} 
\begin{document}
\newcommand{\s}[0]{\sin(\theta)}
\newcommand{\cs}[0]{\cos(\theta)}
\newcommand{\spa}[0]{\hspace{1mm}}
% ------------------------------------------ %
%                 START HERE                 %
% ------------------------------------------ %

\title{MATH 20410 \ PSet 6} % Replace X with the appropriate number
\author{Filippos Tsoukis} % Replace "Author's Name" with your name
\date{\today}
\maketitle

% -----------------------------------------------------
% The following two environments (theorem, proof) are
% where you will enter the statement and proof of your
% first problem for this assignment.
%
% In the theorem environment, you can replace the word
% "theorem" in the \begin and \end commands with
% "exercise", "problem", "lemma", etc., depending on
% what you are submitting. Replace the "x.yz" with the
% appropriate number for your problem.
%
% If your problem does not involve a formal proof, you
% can change the word "proof" in the \begin and \end
% commands with "solution".
% -----------------------------------------------------

\begin{problem}{9.23}
\end{problem}

\begin{solution}
	Begin by a simple computation \[
		f(0,1,-1) = (0)^{2} 1 +e^{0} + (-1) = 0 + 1 - 1 = 0.5
	\]
	The 1st partial derivative wrt to x is \[
		D_{1}f(x,y_{1},y_{2}) = 2xy_{1} + e^{x} + 0; \text{evaluated at the desired point being: } = 0 + 1 = 1 \neq 0 
	\]
	Clearly, this represents an invertible transformation, hence, we have found a point \[
		(x_0, y_0) \text{ for which the derivative is invertible, and the value is 0 }
	\]
	The function, say, $g$ which is defined, by $g(y_0)\coloneqq \text{ the correspoding value of x for which is $(x, y_0)$ is a root of f }$ is $C^1$, in a neighborhood of $y_0$.
	\\
	By the same theorem from which we obtained the previous result, the implicit value theorem, we know that each derivative of this $g$ is given by \[
		D_{1}g(y_1,y_2) = -D_{1,x}(g(y), y)^{-1}D_{1, y}(g(y),y)
	\]
	But this $y$ we are given, we know something about it: \[
		g(y) = 0 ; \text{hence} -D_{1,x}(g(y), y)^{-1}D_{1, y}(g(y),y) = -D_{1,x}(g(y), 0)^{-1}D_{1, y}(g(y),0) = (-h, k)^{T }
	\]


\end{solution}

\begin{problem}{9.24}
\end{problem}
\begin{solution}
	I suppose begin by parameterising, and call the first output $a \text{ and the second }b$. Hence, though the - in front of $y^{2}$ may irritate: \[
		a^{2}+4b^{2} = \frac{x^{4}-2x^{2}y^{2}+y^{4} + 4x^{2}y^{2}}{(x^{2}+y^{2})^{2}} = \frac{x^{4}+2x^{2}y^{2}+y^{4}}{(x^{2}+y^{2})^{2}} = \frac{(x^{2}+y^{2})^{2}}{(x^{2}+y^{2})^{2}} = 1
	\]
	Returning to the original expression, to $a^{2}+4b^{2}=1$, and notice that any value of $a \text{ or } b$ is allowed. Furthermore, pick the fixed variable to be $b$ wlog, and observe that \[
		a = \pm \sqrt{1-4b^{2}} 
	\]
So that range of that is the range of the function.
To compute the rank of the derivative, first compute each partial (and observe they are continuous, and therefore rank makes sense as a notion):
\[
	D_{1}f_{1}(x,y) = \frac{(2x)(x^{2}+y^{2}) - (x^{2})(2x)}{(x^{2}+y^{2})^{2}} = \frac{2xy^{2}}{(x^{2}+y^{2})^{2}}
\]
\[
	D_{2}f_{1}(x,y) = \frac{(2y)(x^{2}+y^{2}) - (y^{2})(2y)}{(x^{2}+y^{2})^{2}} = \frac{2yx^{2}}{(x^{2}+y^{2})^{2}}
\]
\[
	D_{1}f_{2}(x,y) = \frac{(y)(x^{2}+y^{2}) - (xy)(2x)}{(x^{2}+y^{2})^{2}} = \frac{y^{3}-x^{2}y}{(x^{2}+y^{2})^{2}}
\]
\[
		D_{2}f_{2}(x,y) = \text{ same as before, x,y flipped, by symmetry } = \frac{x^{3}-y^{2}x}{(x^{2}+y^{2})^{2}}
\]
\\
Hence, the derivative is the following matrix:
\[
	\begin{matrix}
		\frac{2xy^{2}}{(x^{2}+y^{2})^{2}} &  \frac{2yx^{2}}{(x^{2}+y^{2})^{2}} \\
		\frac{y^{3}-x^{2}y}{(x^{2}+y^{2})^{2}} & \frac{x^{3} - y^{2}x}{(x^{2}+y^{2})^{2}}
	\end{matrix}
\]
Clear by the consistent addition of powers of $x \text{\&} y$ across the diagnonals, the determinant is identically 0. ( I really don't want to write out the whole thing in latex).
So undetermined rank. To be more specific, observe that the determinant is only undefined if $x=y=0$, which we know never to be the case. To observe if the rank is ever 0 (clearly, they are not identically zero, as the left and right columns can be made distinct, try $x=0 \spa y =1$) we can ignore the constant scalar factor $\frac{1}{(x^{2}+y^{2})^2}$. Hence, looking to see if there are are any values $(x,y)$ s.t \[
	\begin{pmatrix}
		2xy^2 \\
		y^{3}-x^{2}y
	\end{pmatrix} = 
	\lambda \begin{pmatrix}
		2yx^2 \\
		x^{3}-y^{2}x
	\end{pmatrix}
\]
\[
\]
Or otherwise:
\[
2xy^{2} = \lambda 2y^{2}x
\]
\[
y^{3}-x^{2}y= \lambda (x^{3}-y^{2}x)\]
Now, this means taht $x = \lambda y $, which added to the second equation means:
\[
	y^{3} - \lambda^{2}y^{3} = \lambda(\lambda^{3}y^{3}- \lambda y^{3})
\]
This surely means that $x=y$, for any such combination. So there, the rank is 1 one otherwise, and 0 in that case. 
\newpage
\end{solution}
\begin{problem}{9.27}
\end{problem}

\begin{solution}
	(a)	Continuity means: \[
		\forall \epsilon > 0 \exists \delta \text{ s.t } |p-q|<\delta \implies ||f(p)-f(q)||<\epsilon    
	\]
	Beginning from the second part, \[
		\frac{p_{1}p_{2}(p_{1}^{2}-p_{2}^{2})}{p_{1}^{2}+p_{1}^{2}}   = \frac{p_1 p_2(p_{1}^{2}-p_{2}^{2})}{p_{1}^{2}+p_{2}^{2}}- \frac{q_1 q_2(q_{1}^{2}-q_{2}^{2})}{q_{1}^{2}+q_{2}^{2}} 
		
	\]
	And clearly since the sum of a 2 squares is greater than the subtraction of them, provided neither is 0, then we get by algebraic manipulation:
	\[
		\text{above}< p_1 p_2 - q_1 q_2
		\]
		We do not even need to discuss why there exists a $\delta$ for which the above is less by $\epsilon$, and where hence the desired inequality is less than $\epsilon$.
\\ \\
In particular, \[
	D_{1}f(x,y) = \frac{(y(x^{2}-y^{2})-xy(2x))(x^{2}+y^{2})- xy(x^{2}-y^{2}))(x^{2}+y^{2})}{(x^{2}+y^{2})^{2}}   
\]
In particular, and ignoring y as it is a constant, we have a rational function, and those are always continuous. We know that $D_{2}f$ is identical, except multiplied by $(-1)$ and flipped wrt $x \& y$, hence for the same reason continuous.

(b) Rational functions, so clearly the functions have derivatives. The derivatives are also rational, so they are also continuous; except of course when the denominator, which is $(x^{2}+y^{2})^{n}$, is 0, which is if and only if $x=y=0$. 
\end{solution}
\begin{problem}{9.29}
\end{problem}
\begin{solution}
	The definition of successive partial derivatives is their successive application: \[
		D_{e_{i}}\dots D_{e_{j}}.
	\]
	Now by the inductive step, we know that whichever derivative is after any derivative upto $k$, can be interchanged. To take \[
		D_{e_{1}}\dots D_{e_{k}}
	\] To any other permutation of the derivatives, define the following algorithm: index the positions from 1 to k in the obvious way. Then, if the $D_{e_{1}}$ partial belongs in the $i_{1}^{th}$ index, do the shuffles as defined by theorem 9.41 $(i_{1}-1)$ times. Then, repeat this for the $nth$ derivatives, performing $i_{n}- \text{ current index}$ shuffles, where a negative number indicates left-ward shuffles. This defines the algorithm, which, by successive applications of 9.41, gets any permutation of $S_{k}$. So done.

\end{solution}

\begin{problem}{6.2}
\end{problem}

\begin{solution}
	We know that if we find one partition whose lower sum is greater than zero, the integral cannot be zero, since the supremum of the lower sums is bounded below by any lower sum. Now assume that there exists a point $t \text{ s.t }  0 \leq f(t) \neq 0$. Now, by continuity, I can find a ball $B_{\delta}(t)$ s.t $f(B_{\delta}(t)) \subset \R ^{+}$. So, pick any partition of $[a, b]$ where, for example, $\exists \text{ i, s.t } x_{i} = t - \frac{\delta}{2} \text{ and where } x_{i+1} = t +\frac{\delta}{2}$. Then
\begin{itemize}
	\item The infimum of the interval $[x_{i}, x_{i+1}]$ is greater than $0$
	\item The infimum of the rest of the sub-intervals of the partion chosen above, by $f\geq 0$ is also \textit{at least} 0. 
\end{itemize}
 So the whole upper sum is greater than 0, so the integral is bounded below by a number greater than 0, so in particular it is not zero. $#$ contradiction of the given value for the integral, so $\nexists t \text{ s.t } f(t) > 0$. Then by the given $f \geq 0$, $f = 0, \forall x \in [a,b]$.
\end{solution}

\begin{problem}{6.4}
\end{problem}
	
\begin{solution}
	(Was this not an in class example?) Riemann integrability means that \[
		\masthscr{U}(\mathscr{P}, f) - \mathscr{L}(\mathscr{P}, f) < \epsilon \text{ for some some partition, regardless of which }\epsilon > 0
	\]
	However, rationals and irrationals are both dense in $\R$, hence any of the intervals implied by the partition always include a rational number, and always include an irrational number. Hence in particular: \[
	\forall x_{i}, \spa x_{i+1}, \spa \exists t_{1} \in [x_{i}, x_{i+1}] \text{ s.t }f(t_{1}) = 1 \text{ (the rational which we have guaranteed) } \]\[ \& \exists t_{2} \in [x_{i}, x_{i+1}] \text{ s.t }f(t_{2}) = 0 \text{ (the rational which we have guaranteed) }.
	\]
	Combining this inclusion of points with output 1 and 0 in any interval, since by the definition of the function, sup of f over the whole domain is $1$ and inf of f over the domain is $0$, these must be exactly the values taken for $sup(\Delta _{x_{i}})$ and $inf(\Delta _{x_{i+1}})$ respectively. Hence, for $[a, b]$,  \[
	\mathscr{U} (\mathscr{P} , f) - \mathscr{L} (\mathscr{P} , f) = 1- 0 = 1
\] Certainly this means that the previous consequence of Riemann integrability does not hold in this case, and hence the function is riemann integrable for no $[a,b]$, provided this even makes sense for Riemann integrability, i.e $a \neq b$. In particular, for $a<b$.
 
 	

\end{solution}

\begin{problem}{6.7}
\end{problem}

\begin{solution}
	Begin by establishing the following steps: 
	\begin{itemize}
		\item The supremum of $[0,\epsilon]$, is greater or equal to the supremum of $[0, \epsilon_{2}]$ when $\epsilon_{2}<\epsilon$.
		\item	Remember that $\int_{0}^{1}f = \int_{0}^{c}f + \int_{c}^{1}f$, when this makes sense
			\item For c as in the stem, $\int_{0}^{c}f \leq c\sup (f([0,c]))$
	\end{itemize}
	So we may continue, calling the supremum of f over $[0,1]$ $\alpha$, and using the 3rd bullet point to to say show that \[
		I_{c} =	 \int_{0}^{c}f \leq \alpha c
	\]
	So certainly $\lim_{c \to 0+}   I_{c} = 0$. So since $\lim_{c \to 0+}  \int_{0}^{1}f = \int_{0}^{1}f$, we write: \[
		\lim_{ c \to 0+}\int_{c}^{1}  f = \lim_{c \to 0+} \int_{0}^{c}f + \lim_{ c\to 0+} \int_{0}^{1}f  = \int_{0}^{1}f        
	\]   RK: Here we have used from the stem that the limit exists
\\
Define f to be $f(x) = 0 \text{ for } x>\frac{1}{2}$, and then $f(x) = \tan((2\pi x) - \frac{\pi}{2})$. This is just the 0 function until $\frac{1}{2}$ and then it is 1 period of tan squeezed into $[0,\frac{1}{2}]$, converted to period of $\frac{1}{2}$. Since it is it $\frac{1}{2}$ periodic and odd, the integral over $[0, \frac{1}{2}]$ is 0. But taking its absolute value makes the function to go to infinity in the domain, and we don't know what to do with that. What we mean is that the limit demanded does not exist.
\end{solution}

\begin{problem}{1}
\end{problem}

\begin{solution}
(a)  The first part is a simple application of the previous problem set question on a special case of the chain rule. In particular: \[
	(f\circ \gamma)`(t) = \langle\ \nabla f(\gamma (t)),\gamma	` (t) \rangle = \langle\ \nabla f(\gamma (t)), y-x \rangle 
\]
%Now, the second part arises from the explicit writing out of the above:
%	\item $ \nabla f(\gamma (t)) = (D_{1}f(\gamma (t)), \dots,  D_{n}f(\gamma (t)))$
%	\item Hence, by the definition of the inner product, and by the chain rule: \\
%	$\langle\ \nabla f(\gamma (t)), y-x \rangle ' = \left(\sum_{k=1}^{n} D_{k}f(\gamma (t)) (y_{k} - x_{k})\right) '$
%	\item TO save time in the computation, remark that in teh $fg'$ part of the cain rule, $g'$ is 1 only for the kth partial, and 0 else. 
	So go again: \[
		(f \circ \gamma (t))''(t) = (\sum_{k=1}^{n} (D_{i}f(\gamma (t))(y_{i}-x_{i})))' = \sum_{k=1}^{n} \nubla (D_{i}f(\gamma (t))(y_{i}-x_{i})+D_{i}f(\gamma (t))\times 0) = \sum_{k=1}^{n} \nubla D_{i}f(\gamma (t))(y_i -x_i)^2   
	\]
	Now, by the definition of matrix multiplication, and of the transpose, the expression in the hypothesis is immediately obtained.
	\\
	(b) To use the previous part, it follows that one might compose $f(x)$ similarly to before. But first we must express $f$ somehow. Luckily, $f \in \mathscr{C}^{2}$, hence we are allowed to taylor approximate the composition of $f and \gamma $, which continuous and from R to R (in particular approximating about t =0, or x):
	\[
		f(\gamma (a)) = f(\gamma (0))) + (f \circ \gamma )' (0) + (f \circ \gamma )'' (t) = f(x) + \langle\ \nabla f(\gamma (0)), y-x \rangle +\frac{1}{2} (y-x) ^{T}\left[D^{2}f(\gamma (t))\right](y-x)
\] Now take a = 1, $\gamma (a) = y$
\[
		f(y) = f(x) +  \langle\ \nabla f(x), y-x \rangle +\frac{1}{2} (y-x) ^{T}\left[D^{2}f(x +t(y-x))\right](y-x)
	\]
Hence, \[
	|f(y) -P_{2}(y)| = \left| \frac{1}{2}(y-x)^{T}\left[D^{2}f(x+t(y-x))+D^{2}f(x)\right] (y-x)\right| \leq \frac{1}{2}\left|\left[D^{2}f(x+t(y-x))+D^{2}f(x)\right] \right| \dot |y-x|^{2}
\] 
Remark that, the above is true for a fixed t. Now, also observe that the middle part is just one linear transformation/matrix (by the closure over addition of invertible matrices). Hence, firstly, the above is bounded by the norm of $y-x$ squared, time some operator norm:
\[
	\frac{|f(y)-P^{2}|}{|y-x|^{2}} \leq \frac{1}{2} \norm{\left[D^{2}f(x+t(y-x))+D^{2}f(x)\right]}  	
\] But the above matrix is just the partials. By the continuity of the partials, as t goes to 0, each of the cells in the nxn matrix goes to 0, and therefore their norm does, in particular by the relation between the operator norm and the frobenius norm. So done.
\\
(c)

Write the function as above, where the input is the convexity between y and x. If a function's 1st derivative is 0, and in particular its 2nd derivative is in all directions positive, then we have a local minimum. Hence, if $\nabla f(x)$ = 0, then the first derivative at $ t = 0 $, is clearly 0 (by definition, one supposes, if we take $y \neq x$, if the inner product is 0). So, in that case, whichever y is chosen, the derivative is 0. Furthermore, by the definition of positive definite, and since $D^{2}f(\gamma (t)) \in \mathbf{R}^{n \times n}$, then the second derivative is positive. This is, again, regardless of which direction (i.e, which y) is chosen.
\end{solution}
\end{document}
	
